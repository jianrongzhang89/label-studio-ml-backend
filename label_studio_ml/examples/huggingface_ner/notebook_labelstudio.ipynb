{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc8cab4",
   "metadata": {},
   "source": [
    "# Workbench and Label Studio Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8209d",
   "metadata": {},
   "source": [
    "First, install the dependencies, including label-studio-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05241b64-faf9-439d-9ad0-536bc36138c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib64/python3.11/site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/29/a2/d40fb2460e883eca5199c62cfc2463fd261f760556ae6290f88488c362c0/pip-25.1.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-25.1.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m196.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m538.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.0 huggingface-hub-0.31.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (2.4.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/app-root/lib64/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (18.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (4.67.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.0b3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: dill\n",
      "\u001b[2K    Found existing installation: dill 0.3.9\n",
      "\u001b[2K    Uninstalling dill-0.3.9:\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.9\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [datasets]3/4\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-3.6.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/app-root/lib64/python3.11/site-packages (from seqeval) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/app-root/lib64/python3.11/site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "\u001b[33m  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=dd0a0ee5a92fa6e329a64cdca826ff7f3d06d04a481e65fc781ac11e11230851\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vo99xb3x/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.0b3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Collecting label-studio-sdk\n",
      "  Downloading label_studio_sdk-1.0.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: Pillow>=10.0.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (11.0.0)\n",
      "Collecting appdirs>=1.4.3 (from label-studio-sdk)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting datamodel-code-generator==0.26.1 (from label-studio-sdk)\n",
      "  Downloading datamodel_code_generator-0.26.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (0.27.2)\n",
      "Collecting ijson>=3.2.3 (from label-studio-sdk)\n",
      "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting jsf<0.12.0,>=0.11.2 (from label-studio-sdk)\n",
      "  Downloading jsf-0.11.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (4.23.0)\n",
      "Collecting lxml>=4.2.5 (from label-studio-sdk)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting nltk<4.0.0,>=3.9.1 (from label-studio-sdk)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.4 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.1.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.2.3)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (1.10.19)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from label-studio-sdk)\n",
      "  Downloading pydantic_core-2.34.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyjwt<3.0.0,>=2.10.1 (from label-studio-sdk)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.32.3)\n",
      "Collecting requests-mock==1.12.1 (from label-studio-sdk)\n",
      "  Downloading requests_mock-1.12.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (4.12.2)\n",
      "Requirement already satisfied: ujson>=5.8.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (5.10.0)\n",
      "Collecting xmljson==0.2.1 (from label-studio-sdk)\n",
      "  Downloading xmljson-0.2.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting argcomplete<4.0,>=1.10 (from datamodel-code-generator==0.26.1->label-studio-sdk)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: black>=19.10b0 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (24.10.0)\n",
      "Collecting genson<2.0,>=1.2.1 (from datamodel-code-generator==0.26.1->label-studio-sdk)\n",
      "  Downloading genson-1.3.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting inflect<6.0,>=4.1.0 (from datamodel-code-generator==0.26.1->label-studio-sdk)\n",
      "  Downloading inflect-5.6.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: isort<6.0,>=4.3.21 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (5.13.2)\n",
      "Requirement already satisfied: jinja2<4.0,>=2.10.1 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (3.1.4)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (24.1)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2<4.0,>=2.10.1->datamodel-code-generator==0.26.1->label-studio-sdk) (3.0.2)\n",
      "Collecting faker>=15.3.4 (from jsf<0.12.0,>=0.11.2->label-studio-sdk)\n",
      "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic>=1.9.2 (from label-studio-sdk)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting rstr>=3.2.0 (from jsf<0.12.0,>=0.11.2->label-studio-sdk)\n",
      "  Downloading rstr-3.2.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: smart-open>=6.3.0 in /opt/app-root/lib64/python3.11/site-packages (from smart-open[http]>=6.3.0->jsf<0.12.0,>=0.11.2->label-studio-sdk) (7.0.5)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (4.67.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.9.2->label-studio-sdk)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from label-studio-sdk)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=1.9.2->label-studio-sdk)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic[email] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting email-validator>=2.0.0 (from pydantic[email]!=2.4.0,<3.0,>=1.10.0; python_version >= \"3.11\" and python_version < \"4.0\"->datamodel-code-generator==0.26.1->label-studio-sdk)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (2024.8.30)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (4.3.6)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.10.0; python_version >= \"3.11\" and python_version < \"4.0\"->datamodel-code-generator==0.26.1->label-studio-sdk) (2.7.0)\n",
      "Requirement already satisfied: tzdata in /opt/app-root/lib64/python3.11/site-packages (from faker>=15.3.4->jsf<0.12.0,>=0.11.2->label-studio-sdk) (2024.2)\n",
      "Requirement already satisfied: anyio in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx>=0.21.2->label-studio-sdk) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas>=0.24.0->label-studio-sdk) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas>=0.24.0->label-studio-sdk) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->label-studio-sdk) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /opt/app-root/lib64/python3.11/site-packages (from smart-open>=6.3.0->smart-open[http]>=6.3.0->jsf<0.12.0,>=0.11.2->label-studio-sdk) (1.17.0rc1)\n",
      "Downloading label_studio_sdk-1.0.12-py3-none-any.whl (390 kB)\n",
      "Downloading datamodel_code_generator-0.26.1-py3-none-any.whl (111 kB)\n",
      "Downloading requests_mock-1.12.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading xmljson-0.2.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading genson-1.3.0-py3-none-any.whl (21 kB)\n",
      "Downloading inflect-5.6.2-py3-none-any.whl (33 kB)\n",
      "Downloading jsf-0.11.2-py3-none-any.whl (49 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m585.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m480.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rstr-3.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: xmljson, ijson, genson, appdirs, typing-inspection, rstr, pyjwt, pydantic-core, nltk, lxml, inflect, faker, email-validator, argcomplete, annotated-types, requests-mock, pydantic, jsf, datamodel-code-generator, label-studio-sdk\n",
      "\u001b[2K  Attempting uninstall: pyjwt\n",
      "\u001b[2K    Found existing installation: PyJWT 2.9.0\n",
      "\u001b[2K    Uninstalling PyJWT-2.9.0:\n",
      "\u001b[2K      Successfully uninstalled PyJWT-2.9.0\n",
      "\u001b[2K  Attempting uninstall: pydantic0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [faker]\n",
      "\u001b[2K    Found existing installation: pydantic 1.10.19━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [faker]\n",
      "\u001b[2K    Uninstalling pydantic-1.10.19:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [faker]\n",
      "\u001b[2K      Successfully uninstalled pydantic-1.10.190m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m16/20\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [label-studio-sdk][label-studio-sdk]nerator]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.23.1 requires pydantic<2, but you have pydantic 2.11.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 appdirs-1.4.4 argcomplete-3.6.2 datamodel-code-generator-0.26.1 email-validator-2.2.0 faker-37.1.0 genson-1.3.0 ijson-3.3.0 inflect-5.6.2 jsf-0.11.2 label-studio-sdk-1.0.12 lxml-5.4.0 nltk-3.9.1 pydantic-2.11.4 pydantic-core-2.33.2 pyjwt-2.10.1 requests-mock-1.12.1 rstr-3.2.2 typing-inspection-0.4.0 xmljson-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets\n",
    "!pip install -q diffusers  peft torch torchvision \n",
    "!pip install -q ipywidgets jupyterlab dataclass_wizard\n",
    "!pip install seqeval\n",
    "!pip install evaluate\n",
    "!pip install label-studio-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbfa99",
   "metadata": {},
   "source": [
    "# Download training data from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4486c53c-3303-4694-be34-e73b3ad6b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'typing' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'typing'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading file traningdata-1000-before.json from [ner-source] directory...\n"
     ]
    }
   ],
   "source": [
    "# Check the following 3 variables before proceeding.\n",
    "import_test_data_from_aws = True # Set to True to import data stored in S3 bucket to the Label Studio project\n",
    "existing_project_id = 0 # If 0, a new project will be created. Otherwise set to the existing Label Studio project ID\n",
    "project_title = 'Huggingface Project' # Title of the Label Studio project. Ignored if existing_project_id > 0.\n",
    "\n",
    "if import_test_data_from_aws:\n",
    "    %run ./transfer-aws.ipynb\n",
    "    project_title = 'Huggingface Project'\n",
    "    prefix = \"ner-source\"  # Directory where the input data file is stored in AWS S3 bucket\n",
    "    input_file='traningdata-1000-before.json'\n",
    "    #test with the sample training data updated by Label Studio \n",
    "    #prefix = \"ner-labelled\"\n",
    "    #input_file='trainingdata-1000-after.json'\n",
    "\n",
    "    s3_env: S3Env = init()\n",
    "    dir_model = BucketMeta(\n",
    "                       bucket_name=s3_env.bucket_name,\n",
    "                       client=s3_env.client,\n",
    "                       file_name=input_file,\n",
    "                       prefix=prefix,\n",
    "                       exclude_dirs_set=['logs'])\n",
    "    download_file(dir_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b2d1e-45f5-4e32-9a06-66ae289a0beb",
   "metadata": {},
   "source": [
    "# Connect to Label Studio and Create a Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60002d1e-323b-4765-aa5c-ebca95a19185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use existing Lable Studio project with ID 36 and title Huggingface Project\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the URL where Label Studio is accessible and the API key for your user account\n",
    "LABEL_STUDIO_URL = os.environ.get('LABEL_STUDIO_URL')\n",
    "# API key is available at the Account & Settings > Access Tokens page in Label Studio UI\n",
    "API_KEY = os.environ.get('API_KEY')\n",
    "\n",
    "# Import the SDK client module\n",
    "from label_studio_sdk import Client\n",
    "from label_studio_sdk.label_interface.create import choices\n",
    "\n",
    "# Connect to the Label Studio Client and check the connection\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "\n",
    "# Label Studio project configuration\n",
    "label_config = \"\"\"\n",
    "<View>\n",
    "  <Labels name=\"label\" toName=\"text\">\n",
    "    <Label value=\"PER\" background=\"red\"/>\n",
    "    <Label value=\"ORG\" background=\"darkorange\"/>\n",
    "    <Label value=\"LOC\" background=\"orange\"/>\n",
    "    <Label value=\"MISC\" background=\"green\"/>\n",
    "  </Labels>\n",
    "  <Text name=\"text\" value=\"$text\"/>\n",
    "</View>\n",
    "    \"\"\"\n",
    "\n",
    "if existing_project_id == 0:\n",
    "    # Create a Label Studio project\n",
    "    project = ls.start_project(\n",
    "      title=project_title,\n",
    "      label_config=label_config,\n",
    "    )\n",
    "    print(f\"Created Lable Studio project {project_title} with ID {project.get_params()['id']}.\")\n",
    "else:\n",
    "    project = ls.get_project(existing_project_id)\n",
    "    project_title = project.get_params()['title']\n",
    "    print(f\"Use existing Lable Studio project with ID {existing_project_id} and title {project_title}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b7816-4a1a-4c68-a29c-212973283f00",
   "metadata": {},
   "source": [
    "# Import Labelled Data from Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e737bb63-a2dc-4a6f-ac61-cb9150bcc8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag names\n",
    "names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "index2tag = {idx:tag for idx, tag in enumerate(names)}\n",
    "tag2index = {tag:idx for idx, tag in enumerate(names)}\n",
    "tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f20308-9de3-4622-96e3-f9854b2b764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_test_data_from_aws:\n",
    "    result=project.import_tasks(tasks=input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc6bf5a-82a4-491d-a771-eb49aec04c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 task(s) exported from Label Studio project\n"
     ]
    }
   ],
   "source": [
    "# After user has done labelling in Label Studio, retrieve that data from Label Studio\n",
    "tasks = project.get_tasks()\n",
    "tasks_count = len(tasks)\n",
    "if tasks_count == 0:\n",
    "    print('No tasks exported from Label Studio project')\n",
    "else:\n",
    "    print(f'{tasks_count} task(s) exported from Label Studio project')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ad0d0-fbd0-43e6-b701-c980d7855382",
   "metadata": {},
   "source": [
    "# Create a tokenizer and data collator for the base NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e041ce8b-ff04-4be8-aca0-4aabcb1d0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "#model_checkpoint = \"distilbert-base-cased\"\n",
    "model_checkpoint = 'dslim/bert-base-NER'\n",
    "#model_checkpoint = \"/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner/checkpoint-5268\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f04a7-3a81-4d78-a6dc-b9efdc5d4eef",
   "metadata": {},
   "source": [
    "# Transform labelled data to a tokenized dataset for NER model fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae60a22-4628-4390-aa3f-3e02794190ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk.label_interface.objects import PredictionValue\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pathlib\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, ClassLabel, Value, Sequence, Features\n",
    "from functools import partial\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def is_valid_url(path):\n",
    "    # Check if the text is a valid URL\n",
    "    try:\n",
    "        result = urlparse(path)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_preload_needed(url):\n",
    "    if url.startswith('upload') or url.startswith('/upload'):\n",
    "        url = '/data' + ('' if url.startswith('/') else '/') + url\n",
    "\n",
    "    is_uploaded_file = url.startswith('/data/upload')\n",
    "    is_local_storage_file = url.startswith('/data/') and '?d=' in url\n",
    "    is_cloud_storage_file = url.startswith('s3:') or url.startswith('gs:') or url.startswith('azure-blob:')\n",
    "    path_exists = os.path.exists(url)\n",
    "\n",
    "    return (\n",
    "        is_uploaded_file\n",
    "        or is_local_storage_file\n",
    "        or is_cloud_storage_file\n",
    "        or is_valid_url(url)\n",
    "        or path_exists\n",
    "    )\n",
    "\n",
    "def preload_task_data(task: Dict, value=None, read_file=True):\n",
    "    \"\"\" Preload task_data values using get_local_path() if values are URI/URL/local path.\n",
    "\n",
    "    Args:\n",
    "        task: Task root.\n",
    "        value: task['data'] if it's None.\n",
    "        read_file: If True, read file content. Otherwise, return file path only.\n",
    "\n",
    "    Returns:\n",
    "        Any: Preloaded task data value.\n",
    "    \"\"\"\n",
    "    # recursively preload dict\n",
    "    if isinstance(value, dict):\n",
    "        for key, item in value.items():\n",
    "            value[key] = preload_task_data(task=task, value=item, read_file=read_file)\n",
    "        return value\n",
    "\n",
    "    # recursively preload list\n",
    "    elif isinstance(value, list):\n",
    "        return [\n",
    "            preload_task_data(task=task, value=item, read_file=read_file)\n",
    "            for item in value\n",
    "        ]\n",
    "\n",
    "    # preload task data if value is URI/URL/local path\n",
    "    elif isinstance(value, str) and is_preload_needed(value):\n",
    "        filepath = self.get_local_path(url=value, task_id=task.get('id'))\n",
    "        if not read_file:\n",
    "            return filepath\n",
    "        with open(filepath, 'r') as f:\n",
    "            return f.read()\n",
    "\n",
    "    # keep value as is\n",
    "    return value\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label =  -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label%2==1:\n",
    "                label = label + 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True,is_split_into_words=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f6dbe0-87fa-4502-9bde-2b10d927d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset[0]: {'id': 16902, 'tokens': ['Inzamam-ul-Haq', ',', 'Salim', 'Malik', ',', 'Asif', 'Mujtaba', ',', 'Wasim', 'Akram', ',', 'Moin'], 'ner_tags': [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "from label_studio_sdk.label_interface import LabelInterface\n",
    "label_interface = LabelInterface(config=label_config)\n",
    "\n",
    "ds_raw = []\n",
    "no_label = 'O'\n",
    "from_name, to_name, value = label_interface.get_first_tag_occurence('Labels', 'Text')\n",
    "for task in tasks:\n",
    "    if task['annotations'] != None:\n",
    "        for annotation in task['annotations']:\n",
    "            if not annotation.get('result'):\n",
    "                continue\n",
    "            spans = [{'label': r['value']['labels'][0], 'start': r['value']['start'], 'end': r['value']['end']} for r in annotation['result']]\n",
    "            spans = sorted(spans, key=lambda x: x['start'])\n",
    "            text = preload_task_data(task, task['data'][value])\n",
    "            # insert tokenizer.pad_token to the unlabeled chunks of the text in-between the labeled spans, as well as to the beginning and end of the text\n",
    "            last_end = 0\n",
    "            all_spans = []\n",
    "            for span in spans:\n",
    "                if last_end < span['start']:\n",
    "                    all_spans.append({'label': no_label, 'start': last_end, 'end': span['start']})\n",
    "                all_spans.append(span)\n",
    "                last_end = span['end']\n",
    "            if last_end < len(text):\n",
    "                all_spans.append({'label': no_label, 'start': last_end, 'end': len(text)})\n",
    "            # now tokenize chunks separately and add them to the dataset\n",
    "            item = {'id': task['id'], 'tokens': [], 'ner_tags': []}\n",
    "            for span in all_spans:\n",
    "                #tokens = tokenizer.tokenize(text[span['start']:span['end']])\n",
    "                tokens = str(text[span['start']:span['end']]).split()\n",
    "                item['tokens'].extend(tokens)\n",
    "                if span['label'] == no_label:\n",
    "                    item['ner_tags'].extend([tag2index[no_label]] * len(tokens))\n",
    "                else:\n",
    "                    label = 'B-' + span['label']\n",
    "                    item['ner_tags'].append(tag2index[label])\n",
    "                    if len(tokens) > 1:\n",
    "                        label = 'I-' + span['label']\n",
    "                        item['ner_tags'].extend([tag2index[label] for _ in range(1, len(tokens))])\n",
    "            ds_raw.append(item)\n",
    "print(\"Dataset[0]:\", ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a97ea25-43f9-4db1-9dd0-3d4eb283d63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25972f967df4b82a59cfe5c1c055861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to huggingface dataset\n",
    "# Define the features of your dataset\n",
    "features = Features({\n",
    "    'id': Value('string'),\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'ner_tags': Sequence(ClassLabel(names=list(tag2index.keys())))})\n",
    "\n",
    "hf_dataset = Dataset.from_list(ds_raw, features=features)\n",
    "tokenized_dataset_from_labelstudio = hf_dataset.map(tokenize_and_align_labels, \n",
    "                                   batched=True,\n",
    "                                   remove_columns=['id', 'tokens', 'ner_tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09797bd2-0fd5-430a-ac69-627696916e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1632\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_from_labelstudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9288e-823c-4a76-a2e7-13e3eeb25ec7",
   "metadata": {},
   "source": [
    "# Prepare tokenized dataset for for model training validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38120518-78a3-4abb-9d6d-c0e0e830fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"conllpp\")  # use published dataset for validation\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ad3836-b751-4550-a387-8a95cafae555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    tag_name = {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "    return tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ef27af-2d9c-4ea3-8b0c-ba5a1c2cc76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data['validation'].features['ner_tags'].feature\n",
    "new_feature = data['validation'].features['ner_tags']\n",
    "label_names = new_feature.feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9841d879-8587-4ed5-9a4a-d84b692410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46a8566-e2a3-463c-a89b-5850664fb543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3ac328-72f3-4d9b-ace1-6520f0aab5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label =  -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label%2==1:\n",
    "                label = label + 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea040121-7bc8-4c3d-a852-087ca54d84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True,is_split_into_words=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    \n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6c2124-1b00-4de0-a9ea-8a33345b9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b373a8-6c41-469f-bcd3-7ac9fcd77a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540b508-8242-4592-b12f-03df16a5b175",
   "metadata": {},
   "source": [
    "# Metrics for model training measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d1da581-2165-4ff1-b92e-6d9c81a635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the whole dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load('seqeval')\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": all_metrics['overall_precision'],\n",
    "            \"recall\": all_metrics['overall_recall'],\n",
    "            \"f1\": all_metrics['overall_f1'],\n",
    "            \"accuracy\": all_metrics['overall_accuracy']}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7863e2-e670-4e5e-83f9-703c1e998495",
   "metadata": {},
   "source": [
    "# Train/fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2421b30-f2c0-484f-b391-3f8b42ddcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, \n",
    "                                                        id2label=index2tag,\n",
    "                                                        label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66d7815-f682-4b61-8685-a74d612465ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100/539349262.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/408 00:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.233710</td>\n",
       "      <td>0.720980</td>\n",
       "      <td>0.769318</td>\n",
       "      <td>0.744365</td>\n",
       "      <td>0.917173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224741</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.787576</td>\n",
       "      <td>0.946133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=408, training_loss=0.3760434318991268, metrics={'train_runtime': 24.232, 'train_samples_per_second': 134.698, 'train_steps_per_second': 16.837, 'total_flos': 66218421733344.0, 'train_loss': 0.3760434318991268, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\"distilbert-finetuned-ner-1\",\n",
    "                         eval_strategy=\"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate=2e-5,\n",
    "                         num_train_epochs=2,\n",
    "                         weight_decay=0.01)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_dataset_from_labelstudio,  \n",
    "                  #train_dataset = tokenized_datasets['train'].select(range(1000)),\n",
    "                  eval_dataset = tokenized_datasets['validation'].select(range(500)),\n",
    "                  data_collator = data_collator,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  tokenizer = tokenizer)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cf77f-8075-4c3d-8a12-0780531520ad",
   "metadata": {},
   "source": [
    "# Checking model predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f2b283f-804f-4ab9-9c04-ebef3075c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9092642),\n",
       "  'word': 'Bill Belichick continues',\n",
       "  'start': 3,\n",
       "  'end': 27},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.5948597),\n",
       "  'word': 'his',\n",
       "  'start': 40,\n",
       "  'end': 43},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.732297),\n",
       "  'word': 'University of North Carolina',\n",
       "  'start': 59,\n",
       "  'end': 87},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.55224925),\n",
       "  'word': '-',\n",
       "  'start': 95,\n",
       "  'end': 96},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9525205),\n",
       "  'word': 'Patriots',\n",
       "  'start': 96,\n",
       "  'end': 104},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.5319824),\n",
       "  'word': 'name',\n",
       "  'start': 150,\n",
       "  'end': 154},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.7978449),\n",
       "  'word': 'Chapel Hill',\n",
       "  'start': 158,\n",
       "  'end': 169},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.8452968),\n",
       "  'word': 'Through Belichick ’ s',\n",
       "  'start': 190,\n",
       "  'end': 209},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.88934946),\n",
       "  'word': 'Tar Heels',\n",
       "  'start': 234,\n",
       "  'end': 243},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.7128538),\n",
       "  'word': 'to',\n",
       "  'start': 271,\n",
       "  'end': 273},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.9845514),\n",
       "  'word': 'LeGarrette Blount Jr.',\n",
       "  'start': 274,\n",
       "  'end': 295},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.60943234),\n",
       "  'word': '##8',\n",
       "  'start': 310,\n",
       "  'end': 311},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.8569711),\n",
       "  'word': 'Blount — whose father',\n",
       "  'start': 365,\n",
       "  'end': 386},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9115988),\n",
       "  'word': 'Patriots',\n",
       "  'start': 402,\n",
       "  'end': 410},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.8445674),\n",
       "  'word': 'Hamilton High School in',\n",
       "  'start': 472,\n",
       "  'end': 495},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.80177546),\n",
       "  'word': 'Chandler, Arizona',\n",
       "  'start': 496,\n",
       "  'end': 513}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204\"\n",
    "token_classifier = pipeline(\"token-classification\", model=checkpoint, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"As Bill Belichick continues to build up his program at the University of North Carolina, the ex-Patriots head coach is looking to bring in a familiar name to Chapel Hill in the years ahead. Through Belichick’s recruiting efforts, the Tar Heels have now extended an offer to LeGarrette Blount Jr. — with the 2028 prospect posting the news on social media Wednesday. Blount — whose father played for the Patriots for four seasons — plays defensive back and wide receiver at Hamilton High School in Chandler, Arizona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8c03ea-68fa-4ec1-86ef-5740bc1ec569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': np.float32(0.752442),\n",
       "  'word': 'Nashua Circuit Court',\n",
       "  'start': 47,\n",
       "  'end': 67},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.91891444),\n",
       "  'word': 'shows two agents throwing Arnuel Marquez Colmenarez to the',\n",
       "  'start': 68,\n",
       "  'end': 126},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.7692687),\n",
       "  'word': 'and handcuffing him on Feb. 20',\n",
       "  'start': 133,\n",
       "  'end': 163},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.6303008),\n",
       "  'word': 'older man using',\n",
       "  'start': 168,\n",
       "  'end': 183},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.76152855),\n",
       "  'word': 'to',\n",
       "  'start': 191,\n",
       "  'end': 193}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"Recently released security camera footage from Nashua Circuit Court shows two agents throwing Arnuel Marquez Colmenarez to the floor and handcuffing him on Feb. 20. An older man using a cane to walk also ended up flat on his back.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
