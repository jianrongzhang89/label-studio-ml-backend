{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc8cab4",
   "metadata": {},
   "source": [
    "# Workbench and Label Studio Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8209d",
   "metadata": {},
   "source": [
    "First, install the dependencies, including label-studio-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05241b64-faf9-439d-9ad0-536bc36138c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib64/python3.11/site-packages (25.1.1)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib64/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib64/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (2.4.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/app-root/lib64/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib64/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib64/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.0b3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: seqeval in /opt/app-root/lib64/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/app-root/lib64/python3.11/site-packages (from seqeval) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/app-root/lib64/python3.11/site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: evaluate in /opt/app-root/lib64/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.0b3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: label-studio-sdk in /opt/app-root/lib64/python3.11/site-packages (1.0.12)\n",
      "Requirement already satisfied: Pillow>=10.0.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (11.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (1.4.4)\n",
      "Requirement already satisfied: datamodel-code-generator==0.26.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (0.26.1)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (0.27.2)\n",
      "Requirement already satisfied: ijson>=3.2.3 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (3.3.0)\n",
      "Requirement already satisfied: jsf<0.12.0,>=0.11.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (0.11.2)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (4.23.0)\n",
      "Requirement already satisfied: lxml>=4.2.5 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (5.4.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.9.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (3.9.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.4 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.1.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.2.3)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.11.4)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.33.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.10.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (2.32.3)\n",
      "Requirement already satisfied: requests-mock==1.12.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (1.12.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (4.12.2)\n",
      "Requirement already satisfied: ujson>=5.8.0 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (5.10.0)\n",
      "Requirement already satisfied: xmljson==0.2.1 in /opt/app-root/lib64/python3.11/site-packages (from label-studio-sdk) (0.2.1)\n",
      "Requirement already satisfied: argcomplete<4.0,>=1.10 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (3.6.2)\n",
      "Requirement already satisfied: black>=19.10b0 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (24.10.0)\n",
      "Requirement already satisfied: genson<2.0,>=1.2.1 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (1.3.0)\n",
      "Requirement already satisfied: inflect<6.0,>=4.1.0 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (5.6.2)\n",
      "Requirement already satisfied: isort<6.0,>=4.3.21 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (5.13.2)\n",
      "Requirement already satisfied: jinja2<4.0,>=2.10.1 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (3.1.4)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (24.1)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/app-root/lib64/python3.11/site-packages (from datamodel-code-generator==0.26.1->label-studio-sdk) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2<4.0,>=2.10.1->datamodel-code-generator==0.26.1->label-studio-sdk) (3.0.2)\n",
      "Requirement already satisfied: faker>=15.3.4 in /opt/app-root/lib64/python3.11/site-packages (from jsf<0.12.0,>=0.11.2->label-studio-sdk) (37.1.0)\n",
      "Requirement already satisfied: rstr>=3.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsf<0.12.0,>=0.11.2->label-studio-sdk) (3.2.2)\n",
      "Requirement already satisfied: smart-open>=6.3.0 in /opt/app-root/lib64/python3.11/site-packages (from smart-open[http]>=6.3.0->jsf<0.12.0,>=0.11.2->label-studio-sdk) (7.0.5)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from nltk<4.0.0,>=3.9.1->label-studio-sdk) (4.67.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic>=1.9.2->label-studio-sdk) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic>=1.9.2->label-studio-sdk) (0.4.0)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic[email]!=2.4.0,<3.0,>=1.10.0; python_version >= \"3.11\" and python_version < \"4.0\"->datamodel-code-generator==0.26.1->label-studio-sdk) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.22.0->label-studio-sdk) (2024.8.30)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/app-root/lib64/python3.11/site-packages (from black>=19.10b0->datamodel-code-generator==0.26.1->label-studio-sdk) (4.3.6)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.10.0; python_version >= \"3.11\" and python_version < \"4.0\"->datamodel-code-generator==0.26.1->label-studio-sdk) (2.7.0)\n",
      "Requirement already satisfied: tzdata in /opt/app-root/lib64/python3.11/site-packages (from faker>=15.3.4->jsf<0.12.0,>=0.11.2->label-studio-sdk) (2024.2)\n",
      "Requirement already satisfied: anyio in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from httpx>=0.21.2->label-studio-sdk) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx>=0.21.2->label-studio-sdk) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema>=4.23.0->label-studio-sdk) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas>=0.24.0->label-studio-sdk) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas>=0.24.0->label-studio-sdk) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->label-studio-sdk) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /opt/app-root/lib64/python3.11/site-packages (from smart-open>=6.3.0->smart-open[http]>=6.3.0->jsf<0.12.0,>=0.11.2->label-studio-sdk) (1.17.0rc1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets\n",
    "!pip install -q diffusers  peft torch torchvision \n",
    "!pip install -q ipywidgets jupyterlab dataclass_wizard\n",
    "!pip install seqeval\n",
    "!pip install evaluate\n",
    "!pip install label-studio-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbfa99",
   "metadata": {},
   "source": [
    "# Download training data from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4486c53c-3303-4694-be34-e73b3ad6b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file trainingdata-1000-before.json from [ner-source] directory...\n"
     ]
    }
   ],
   "source": [
    "# Check the following 3 variables before proceeding.\n",
    "import_test_data_from_aws = True # Set to True to import data stored in S3 bucket to the Label Studio project\n",
    "existing_project_id = 0 # If 0, a new project will be created. Otherwise set to the existing Label Studio project ID\n",
    "project_title = 'Huggingface Project' # Title of the Label Studio project. Ignored if existing_project_id > 0.\n",
    "\n",
    "if import_test_data_from_aws:\n",
    "    %run ./transfer-aws.ipynb\n",
    "    project_title = 'Huggingface Project'\n",
    "    prefix = \"ner-source\"  # Directory where the input data file is stored in AWS S3 bucket\n",
    "    input_file='trainingdata-1000-before.json'\n",
    "    #test with the sample training data updated by Label Studio \n",
    "    #prefix = \"ner-labelled\"\n",
    "    #input_file='trainingdata-1000-after.json'\n",
    "\n",
    "    s3_env: S3Env = init()\n",
    "    dir_model = BucketMeta(\n",
    "                       bucket_name=s3_env.bucket_name,\n",
    "                       client=s3_env.client,\n",
    "                       file_name=input_file,\n",
    "                       prefix=prefix,\n",
    "                       exclude_dirs_set=['logs'])\n",
    "    download_file(dir_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b2d1e-45f5-4e32-9a06-66ae289a0beb",
   "metadata": {},
   "source": [
    "# Connect to Label Studio and Create a Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60002d1e-323b-4765-aa5c-ebca95a19185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Lable Studio project Huggingface Project with ID 39.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the URL where Label Studio is accessible and the API key for your user account\n",
    "LABEL_STUDIO_URL = os.environ.get('LABEL_STUDIO_URL')\n",
    "# API key is available at the Account & Settings > Access Tokens page in Label Studio UI\n",
    "API_KEY = os.environ.get('API_KEY')\n",
    "\n",
    "# Import the SDK client module\n",
    "from label_studio_sdk import Client\n",
    "from label_studio_sdk.label_interface.create import choices\n",
    "\n",
    "# Connect to the Label Studio Client and check the connection\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "\n",
    "# Label Studio project configuration\n",
    "label_config = \"\"\"\n",
    "<View>\n",
    "  <Labels name=\"label\" toName=\"text\">\n",
    "    <Label value=\"PER\" background=\"red\"/>\n",
    "    <Label value=\"ORG\" background=\"darkorange\"/>\n",
    "    <Label value=\"LOC\" background=\"orange\"/>\n",
    "    <Label value=\"MISC\" background=\"green\"/>\n",
    "  </Labels>\n",
    "  <Text name=\"text\" value=\"$text\"/>\n",
    "</View>\n",
    "    \"\"\"\n",
    "\n",
    "if existing_project_id == 0:\n",
    "    # Create a Label Studio project\n",
    "    project = ls.start_project(\n",
    "      title=project_title,\n",
    "      label_config=label_config,\n",
    "    )\n",
    "    print(f\"Created Lable Studio project {project_title} with ID {project.get_params()['id']}.\")\n",
    "else:\n",
    "    project = ls.get_project(existing_project_id)\n",
    "    project_title = project.get_params()['title']\n",
    "    print(f\"Use existing Lable Studio project with ID {existing_project_id} and title {project_title}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b7816-4a1a-4c68-a29c-212973283f00",
   "metadata": {},
   "source": [
    "# Import Labelled Data from Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e737bb63-a2dc-4a6f-ac61-cb9150bcc8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag names\n",
    "names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "index2tag = {idx:tag for idx, tag in enumerate(names)}\n",
    "tag2index = {tag:idx for idx, tag in enumerate(names)}\n",
    "tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f20308-9de3-4622-96e3-f9854b2b764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_test_data_from_aws:\n",
    "    result=project.import_tasks(tasks=input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc6bf5a-82a4-491d-a771-eb49aec04c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 task(s) exported from Label Studio project\n"
     ]
    }
   ],
   "source": [
    "# After user has done labelling in Label Studio, retrieve that data from Label Studio\n",
    "tasks = project.get_tasks()\n",
    "tasks_count = len(tasks)\n",
    "if tasks_count == 0:\n",
    "    print('No tasks exported from Label Studio project')\n",
    "else:\n",
    "    print(f'{tasks_count} task(s) exported from Label Studio project')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ad0d0-fbd0-43e6-b701-c980d7855382",
   "metadata": {},
   "source": [
    "# Create a tokenizer and data collator for the base NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e041ce8b-ff04-4be8-aca0-4aabcb1d0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "#model_checkpoint = \"distilbert-base-cased\"\n",
    "model_checkpoint = 'dslim/bert-base-NER'\n",
    "#model_checkpoint = \"/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner/checkpoint-5268\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f04a7-3a81-4d78-a6dc-b9efdc5d4eef",
   "metadata": {},
   "source": [
    "# Transform labelled data to a tokenized dataset for NER model fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae60a22-4628-4390-aa3f-3e02794190ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk.label_interface.objects import PredictionValue\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pathlib\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, ClassLabel, Value, Sequence, Features\n",
    "from functools import partial\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def is_valid_url(path):\n",
    "    # Check if the text is a valid URL\n",
    "    try:\n",
    "        result = urlparse(path)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_preload_needed(url):\n",
    "    if url.startswith('upload') or url.startswith('/upload'):\n",
    "        url = '/data' + ('' if url.startswith('/') else '/') + url\n",
    "\n",
    "    is_uploaded_file = url.startswith('/data/upload')\n",
    "    is_local_storage_file = url.startswith('/data/') and '?d=' in url\n",
    "    is_cloud_storage_file = url.startswith('s3:') or url.startswith('gs:') or url.startswith('azure-blob:')\n",
    "    path_exists = os.path.exists(url)\n",
    "\n",
    "    return (\n",
    "        is_uploaded_file\n",
    "        or is_local_storage_file\n",
    "        or is_cloud_storage_file\n",
    "        or is_valid_url(url)\n",
    "        or path_exists\n",
    "    )\n",
    "\n",
    "def preload_task_data(task: Dict, value=None, read_file=True):\n",
    "    \"\"\" Preload task_data values using get_local_path() if values are URI/URL/local path.\n",
    "\n",
    "    Args:\n",
    "        task: Task root.\n",
    "        value: task['data'] if it's None.\n",
    "        read_file: If True, read file content. Otherwise, return file path only.\n",
    "\n",
    "    Returns:\n",
    "        Any: Preloaded task data value.\n",
    "    \"\"\"\n",
    "    # recursively preload dict\n",
    "    if isinstance(value, dict):\n",
    "        for key, item in value.items():\n",
    "            value[key] = preload_task_data(task=task, value=item, read_file=read_file)\n",
    "        return value\n",
    "\n",
    "    # recursively preload list\n",
    "    elif isinstance(value, list):\n",
    "        return [\n",
    "            preload_task_data(task=task, value=item, read_file=read_file)\n",
    "            for item in value\n",
    "        ]\n",
    "\n",
    "    # preload task data if value is URI/URL/local path\n",
    "    elif isinstance(value, str) and is_preload_needed(value):\n",
    "        filepath = self.get_local_path(url=value, task_id=task.get('id'))\n",
    "        if not read_file:\n",
    "            return filepath\n",
    "        with open(filepath, 'r') as f:\n",
    "            return f.read()\n",
    "\n",
    "    # keep value as is\n",
    "    return value\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label =  -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label%2==1:\n",
    "                label = label + 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True,is_split_into_words=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f6dbe0-87fa-4502-9bde-2b10d927d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset[0]: {'id': 20903, 'tokens': ['Inzamam-ul-Haq', ',', 'Salim', 'Malik', ',', 'Asif', 'Mujtaba', ',', 'Wasim', 'Akram', ',', 'Moin'], 'ner_tags': [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "from label_studio_sdk.label_interface import LabelInterface\n",
    "label_interface = LabelInterface(config=label_config)\n",
    "\n",
    "ds_raw = []\n",
    "no_label = 'O'\n",
    "from_name, to_name, value = label_interface.get_first_tag_occurence('Labels', 'Text')\n",
    "for task in tasks:\n",
    "    if task['annotations'] != None:\n",
    "        for annotation in task['annotations']:\n",
    "            if not annotation.get('result'):\n",
    "                continue\n",
    "            spans = [{'label': r['value']['labels'][0], 'start': r['value']['start'], 'end': r['value']['end']} for r in annotation['result']]\n",
    "            spans = sorted(spans, key=lambda x: x['start'])\n",
    "            text = preload_task_data(task, task['data'][value])\n",
    "            # insert tokenizer.pad_token to the unlabeled chunks of the text in-between the labeled spans, as well as to the beginning and end of the text\n",
    "            last_end = 0\n",
    "            all_spans = []\n",
    "            for span in spans:\n",
    "                if last_end < span['start']:\n",
    "                    all_spans.append({'label': no_label, 'start': last_end, 'end': span['start']})\n",
    "                all_spans.append(span)\n",
    "                last_end = span['end']\n",
    "            if last_end < len(text):\n",
    "                all_spans.append({'label': no_label, 'start': last_end, 'end': len(text)})\n",
    "            # now tokenize chunks separately and add them to the dataset\n",
    "            item = {'id': task['id'], 'tokens': [], 'ner_tags': []}\n",
    "            for span in all_spans:\n",
    "                #tokens = tokenizer.tokenize(text[span['start']:span['end']])\n",
    "                tokens = str(text[span['start']:span['end']]).split()\n",
    "                item['tokens'].extend(tokens)\n",
    "                if span['label'] == no_label:\n",
    "                    item['ner_tags'].extend([tag2index[no_label]] * len(tokens))\n",
    "                else:\n",
    "                    label = 'B-' + span['label']\n",
    "                    item['ner_tags'].append(tag2index[label])\n",
    "                    if len(tokens) > 1:\n",
    "                        label = 'I-' + span['label']\n",
    "                        item['ner_tags'].extend([tag2index[label] for _ in range(1, len(tokens))])\n",
    "            ds_raw.append(item)\n",
    "print(\"Dataset[0]:\", ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a97ea25-43f9-4db1-9dd0-3d4eb283d63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002a2275ac60448f9ea16d9026b71ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to huggingface dataset\n",
    "# Define the features of your dataset\n",
    "features = Features({\n",
    "    'id': Value('string'),\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'ner_tags': Sequence(ClassLabel(names=list(tag2index.keys())))})\n",
    "\n",
    "hf_dataset = Dataset.from_list(ds_raw, features=features)\n",
    "tokenized_dataset_from_labelstudio = hf_dataset.map(tokenize_and_align_labels, \n",
    "                                   batched=True,\n",
    "                                   remove_columns=['id', 'tokens', 'ner_tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09797bd2-0fd5-430a-ac69-627696916e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 816\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_from_labelstudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9288e-823c-4a76-a2e7-13e3eeb25ec7",
   "metadata": {},
   "source": [
    "# Prepare tokenized dataset for for model training validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38120518-78a3-4abb-9d6d-c0e0e830fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"conllpp\")  # use published dataset for validation\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ad3836-b751-4550-a387-8a95cafae555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    tag_name = {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "    return tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ef27af-2d9c-4ea3-8b0c-ba5a1c2cc76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data['validation'].features['ner_tags'].feature\n",
    "new_feature = data['validation'].features['ner_tags']\n",
    "label_names = new_feature.feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9841d879-8587-4ed5-9a4a-d84b692410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46a8566-e2a3-463c-a89b-5850664fb543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3ac328-72f3-4d9b-ace1-6520f0aab5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label =  -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label%2==1:\n",
    "                label = label + 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea040121-7bc8-4c3d-a852-087ca54d84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True,is_split_into_words=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    \n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6c2124-1b00-4de0-a9ea-8a33345b9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b373a8-6c41-469f-bcd3-7ac9fcd77a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540b508-8242-4592-b12f-03df16a5b175",
   "metadata": {},
   "source": [
    "# Metrics for model training measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d1da581-2165-4ff1-b92e-6d9c81a635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the whole dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load('seqeval')\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": all_metrics['overall_precision'],\n",
    "            \"recall\": all_metrics['overall_recall'],\n",
    "            \"f1\": all_metrics['overall_f1'],\n",
    "            \"accuracy\": all_metrics['overall_accuracy']}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7863e2-e670-4e5e-83f9-703c1e998495",
   "metadata": {},
   "source": [
    "# Train/fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2421b30-f2c0-484f-b391-3f8b42ddcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, \n",
    "                                                        id2label=index2tag,\n",
    "                                                        label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66d7815-f682-4b61-8685-a74d612465ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2714/539349262.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 00:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226105</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.703858</td>\n",
       "      <td>0.935049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.170038</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.776280</td>\n",
       "      <td>0.951019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=204, training_loss=0.6817282134411382, metrics={'train_runtime': 14.8315, 'train_samples_per_second': 110.036, 'train_steps_per_second': 13.755, 'total_flos': 33178622210208.0, 'train_loss': 0.6817282134411382, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\"distilbert-finetuned-ner-1\",\n",
    "                         eval_strategy=\"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate=2e-5,\n",
    "                         num_train_epochs=2,\n",
    "                         weight_decay=0.01)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_dataset_from_labelstudio,  \n",
    "                  #train_dataset = tokenized_datasets['train'].select(range(1000)),\n",
    "                  eval_dataset = tokenized_datasets['validation'].select(range(500)),\n",
    "                  data_collator = data_collator,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  tokenizer = tokenizer)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cf77f-8075-4c3d-8a12-0780531520ad",
   "metadata": {},
   "source": [
    "# Checking model predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f2b283f-804f-4ab9-9c04-ebef3075c83f",
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m token_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m token_classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs Bill Belichick continues to build up his program at the University of North Carolina, the ex-Patriots head coach is looking to bring in a familiar name to Chapel Hill in the years ahead. Through Belichick’s recruiting efforts, the Tar Heels have now extended an offer to LeGarrette Blount Jr. — with the 2028 prospect posting the news on social media Wednesday. Blount — whose father played for the Patriots for four seasons — plays defensive back and wide receiver at Hamilton High School in Chandler, Arizona.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/pipelines/__init__.py:812\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/utils/hub.py:470\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull_filenames\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/utils/hub.py:471\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    470\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 471\u001b[0m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    472\u001b[0m ]\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/utils/hub.py:134\u001b[0m, in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cache_file_to_return\u001b[39m(\n\u001b[1;32m    131\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m, full_filename: \u001b[38;5;28mstr\u001b[39m, cache_dir: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m ):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file \u001b[38;5;241m!=\u001b[39m _CACHED_NO_EXIST:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"/opt/app-root/src/label-studio-ml-backend/label_studio_ml/examples/huggingface_ner/distilbert-finetuned-ner-1/checkpoint-204\"\n",
    "token_classifier = pipeline(\"token-classification\", model=checkpoint, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"As Bill Belichick continues to build up his program at the University of North Carolina, the ex-Patriots head coach is looking to bring in a familiar name to Chapel Hill in the years ahead. Through Belichick’s recruiting efforts, the Tar Heels have now extended an offer to LeGarrette Blount Jr. — with the 2028 prospect posting the news on social media Wednesday. Blount — whose father played for the Patriots for four seasons — plays defensive back and wide receiver at Hamilton High School in Chandler, Arizona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c03ea-68fa-4ec1-86ef-5740bc1ec569",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_classifier(\"Recently released security camera footage from Nashua Circuit Court shows two agents throwing Arnuel Marquez Colmenarez to the floor and handcuffing him on Feb. 20. An older man using a cane to walk also ended up flat on his back.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
